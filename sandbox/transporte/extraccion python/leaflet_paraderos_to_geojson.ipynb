{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94a284ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Using cached selenium-4.39.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting webdriver-manager\n",
      "  Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\anaconda\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: requests in c:\\anaconda\\lib\\site-packages (2.32.3)\n",
      "Collecting urllib3<3.0,>=2.5.0 (from urllib3[socks]<3.0,>=2.5.0->selenium)\n",
      "  Using cached urllib3-2.6.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting trio<1.0,>=0.31.0 (from selenium)\n",
      "  Using cached trio-0.32.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
      "  Using cached trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting certifi>=2025.10.5 (from selenium)\n",
      "  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting typing_extensions<5.0,>=4.15.0 (from selenium)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in c:\\anaconda\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\anaconda\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (24.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\anaconda\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\anaconda\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (3.7)\n",
      "Collecting outcome (from trio<1.0,>=0.31.0->selenium)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\anaconda\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\anaconda\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\anaconda\\lib\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.2)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\anaconda\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\anaconda\\lib\\site-packages (from webdriver-manager) (1.1.0)\n",
      "Requirement already satisfied: packaging in c:\\anaconda\\lib\\site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\anaconda\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\anaconda\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: pycparser in c:\\anaconda\\lib\\site-packages (from cffi>=1.14->trio<1.0,>=0.31.0->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.16.0 in c:\\anaconda\\lib\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
      "Using cached selenium-4.39.0-py3-none-any.whl (9.7 MB)\n",
      "Using cached trio-0.32.0-py3-none-any.whl (512 kB)\n",
      "Using cached trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached urllib3-2.6.1-py3-none-any.whl (131 kB)\n",
      "Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: urllib3, typing_extensions, outcome, certifi, trio, webdriver-manager, trio-websocket, selenium\n",
      "\n",
      "  Attempting uninstall: urllib3\n",
      "\n",
      "    Found existing installation: urllib3 2.3.0\n",
      "\n",
      "    Uninstalling urllib3-2.3.0:\n",
      "\n",
      "      Successfully uninstalled urllib3-2.3.0\n",
      "\n",
      "   ---------------------------------------- 0/8 [urllib3]\n",
      "   ---------------------------------------- 0/8 [urllib3]\n",
      "  Attempting uninstall: typing_extensions\n",
      "   ---------------------------------------- 0/8 [urllib3]\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "   ---------------------------------------- 0/8 [urllib3]\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "   ---------------------------------------- 0/8 [urllib3]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ---------- ----------------------------- 2/8 [outcome]\n",
      "  Attempting uninstall: certifi\n",
      "   ---------- ----------------------------- 2/8 [outcome]\n",
      "    Found existing installation: certifi 2025.8.3\n",
      "   ---------- ----------------------------- 2/8 [outcome]\n",
      "    Uninstalling certifi-2025.8.3:\n",
      "   ---------- ----------------------------- 2/8 [outcome]\n",
      "      Successfully uninstalled certifi-2025.8.3\n",
      "   ---------- ----------------------------- 2/8 [outcome]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   ------------------------- -------------- 5/8 [webdriver-manager]\n",
      "   ------------------------------ --------- 6/8 [trio-websocket]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ---------------------------------------- 8/8 [selenium]\n",
      "\n",
      "Successfully installed certifi-2025.11.12 outcome-1.3.0.post0 selenium-4.39.0 trio-0.32.0 trio-websocket-0.12.2 typing_extensions-4.15.0 urllib3-2.6.1 webdriver-manager-4.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~rllib3 (c:\\Anaconda\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rllib3 (c:\\Anaconda\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rllib3 (c:\\Anaconda\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install selenium webdriver-manager beautifulsoup4 requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9815393d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uso:\n",
      "  python wikiroutes_selenium.py catalog --city lima --lang es --out data_wikiroutes --download-assets 1\n",
      "  python wikiroutes_selenium.py route --url 'https://wikiroutes.info/es/lima?routes=154193' --out data_wikiroutes --download-assets 1\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "WikiRoutes scraper con Selenium (scrapeo TOTAL de página de ruta o catálogo).\n",
    "\n",
    "Genera por ruta (carpeta route_<id>/):\n",
    "- page.html ............................. HTML de la página de la ruta (renderizada)\n",
    "- assets/ ................................ Descarga de CSS/JS/IMG referenciados (opcional)\n",
    "- assets_links.txt ....................... Listado de assets detectados\n",
    "- route.json ............................. Metadatos extraídos (id, ref, nombre, operador, etc.)\n",
    "- stops.csv .............................. Tabla de paraderos (orden, sentido, id, nombre, url, lat, lon)\n",
    "- stops.geojson .......................... Puntos de paraderos (si hay lat/lon)\n",
    "- line_approx.geojson .................... LineString de la ruta (preferencia: polyline detectada; fallback: unir paraderos)\n",
    "- raw_coords.json ........................ Candidatos de arrays de coordenadas detectados en scripts (debug)\n",
    "- stops_html/stop_<id>.html .............. HTML de cada parada (si tiene URL)\n",
    "\n",
    "CLI:\n",
    "  python wikiroutes_selenium.py catalog --city lima --lang es --out data_wikiroutes --max-pages 30\n",
    "  python wikiroutes_selenium.py route --url \"https://wikiroutes.info/es/lima?routes=154193\" --out data_wikiroutes\n",
    "\n",
    "Parámetros útiles:\n",
    "  --headless 1|0            Ejecuta Chrome en segundo plano (1 por defecto)\n",
    "  --download-assets 1|0     Descarga assets referenciados por la página (0 por defecto)\n",
    "  --delay 1.2               Retardo entre requests a paradas (segundos)\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import argparse\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, Dict, Any\n",
    "from urllib.parse import urlparse, urljoin, parse_qs\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "DEFAULT_BASE = \"https://wikiroutes.info\"\n",
    "UA = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"\n",
    "REQUEST_DELAY_S = 1.2\n",
    "PAGE_TIMEOUT_S = 30\n",
    "ASSET_TIMEOUT_S = 20\n",
    "MAX_ASSET_SIZE_MB = 15\n",
    "\n",
    "# Selectores y patrones\n",
    "SEL_STOP_BLOCK = \".stops-list-block\"\n",
    "SEL_STOP_HEAD_SMALL = \".stops-list-block-head small\"\n",
    "SEL_STOP_ITEM = \".stops-list-item\"\n",
    "SEL_ROUTE_TITLE = \".MEcFqLPlaQKg.RSZfWQHoH\"\n",
    "SEL_ROUTE_TITLE_FALL = \"h1, .route-title, .header-title, .content-title\"\n",
    "SEL_CITY_LABEL = \".vGyZhDoaGCm.khuKVSRut\"\n",
    "SEL_CITY_LABEL_FALL = \".breadcrumbs a[href*='/es/'], nav a[href*='/es/']\"\n",
    "SEL_LAST_EDIT = \".PNkzKgEzLcTwnP\"\n",
    "SEL_STATS = \".bLKuCSlgiB .MuinWLFvyLRChv\"\n",
    "\n",
    "COORD_PATTERNS = [\n",
    "    re.compile(r\"LatLng\\(\\s*([-0-9\\.]+)\\s*,\\s*([-0-9\\.]+)\\s*\\)\", re.I),\n",
    "    re.compile(r\"['\\\"]lat['\\\"]\\s*[:=]\\s*([-0-9\\.]+)\\s*,\\s*['\\\"](lon|lng)['\\\"]\\s*[:=]\\s*([-0-9\\.]+)\", re.I),\n",
    "    re.compile(r\"data-lat=\\\"([-0-9\\.]+)\\\".*?data-lon=\\\"([-0-9\\.]+)\\\"\", re.I | re.S),\n",
    "]\n",
    "\n",
    "# Para capturar arrays grandes de coords en scripts: [[-12.0,-77.0], [...], ...]\n",
    "BIG_COORD_ARRAY = re.compile(\n",
    "    r\"\\[\\s*\\[\\s*-?\\d+\\.\\d+\\s*,\\s*-?\\d+\\.\\d+\\s*\\](?:\\s*,\\s*\\[\\s*-?\\d+\\.\\d+\\s*,\\s*-?\\d+\\.\\d+\\s*\\]){5,}\\s*\\]\"\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class RouteStop:\n",
    "    sequence: int\n",
    "    direction: str\n",
    "    stop_id: Optional[str]\n",
    "    stop_name: str\n",
    "    stop_url: Optional[str]\n",
    "    lat: Optional[float] = None\n",
    "    lon: Optional[float] = None\n",
    "\n",
    "@dataclass\n",
    "class RouteData:\n",
    "    route_id: Optional[str]\n",
    "    ref: Optional[str]\n",
    "    name: Optional[str]\n",
    "    operator: Optional[str]\n",
    "    last_edit: Optional[str]\n",
    "    stats_raw: List[str]\n",
    "    city: Optional[str]\n",
    "    url: str\n",
    "\n",
    "# -----------------------------\n",
    "# Utilidades web/robots/assets\n",
    "# -----------------------------\n",
    "def robots_allows(base: str, path: str) -> bool:\n",
    "    try:\n",
    "        r = requests.get(urljoin(base, \"/robots.txt\"), timeout=10)\n",
    "        if r.status_code != 200:\n",
    "            return True\n",
    "        ua = \"*\"\n",
    "        dis = []\n",
    "        current = None\n",
    "        for line in r.text.splitlines():\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "            if line.lower().startswith(\"user-agent:\"):\n",
    "                current = line.split(\":\", 1)[1].strip()\n",
    "            elif line.lower().startswith(\"disallow:\") and (current == ua or current == \"*\"):\n",
    "                dis.append(line.split(\":\", 1)[1].strip())\n",
    "        return not any(path.startswith(d) for d in dis if d)\n",
    "    except Exception:\n",
    "        return True\n",
    "\n",
    "def session_headers() -> Dict[str,str]:\n",
    "    return {\n",
    "        \"User-Agent\": UA,\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"es-ES,es;q=0.9,en;q=0.8\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "    }\n",
    "\n",
    "def safe_get(url: str, timeout: int = ASSET_TIMEOUT_S) -> Optional[requests.Response]:\n",
    "    try:\n",
    "        r = requests.get(url, headers=session_headers(), timeout=timeout, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            size_mb = int(r.headers.get(\"Content-Length\", \"0\")) / (1024 * 1024)\n",
    "            if size_mb and size_mb > MAX_ASSET_SIZE_MB:\n",
    "                return None\n",
    "            return r\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def collect_asset_urls(soup: BeautifulSoup, base_url: str) -> List[str]:\n",
    "    urls = set()\n",
    "    # CSS\n",
    "    for link in soup.select(\"link[href]\"):\n",
    "        href = link.get(\"href\")\n",
    "        if href:\n",
    "            urls.add(urljoin(base_url, href))\n",
    "    # JS\n",
    "    for sc in soup.select(\"script[src]\"):\n",
    "        src = sc.get(\"src\")\n",
    "        if src:\n",
    "            urls.add(urljoin(base_url, src))\n",
    "    # Imágenes\n",
    "    for img in soup.select(\"img[src]\"):\n",
    "        src = img.get(\"src\")\n",
    "        if src:\n",
    "            urls.add(urljoin(base_url, src))\n",
    "    return sorted(urls)\n",
    "\n",
    "def download_assets(urls: List[str], out_dir: Path) -> None:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for u in urls:\n",
    "        try:\n",
    "            r = safe_get(u)\n",
    "            if not r:\n",
    "                continue\n",
    "            filename = re.sub(r\"[^\\w\\-.]\", \"_\", urlparse(u).path.split(\"/\")[-1] or \"file\")\n",
    "            # separa por tipo\n",
    "            sub = \"img\"\n",
    "            if filename.endswith((\".css\",)): sub = \"css\"\n",
    "            elif filename.endswith((\".js\",)): sub = \"js\"\n",
    "            target = out_dir / sub\n",
    "            target.mkdir(parents=True, exist_ok=True)\n",
    "            with open(target / filename, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=65536):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "# -----------------------------\n",
    "# Selenium helpers\n",
    "# -----------------------------\n",
    "def make_driver(headless: bool = True, lang: str = \"es-ES\") -> webdriver.Chrome:\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(\"--window-size=1400,1000\")\n",
    "    opts.add_argument(f\"--user-agent={UA}\")\n",
    "    opts.add_argument(f\"--lang={lang}\")\n",
    "    opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "    driver.set_page_load_timeout(PAGE_TIMEOUT_S)\n",
    "    return driver\n",
    "\n",
    "def wait_css(driver: webdriver.Chrome, css: str, timeout: int = PAGE_TIMEOUT_S):\n",
    "    WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.CSS_SELECTOR, css)))\n",
    "\n",
    "def click_if_present(driver: webdriver.Chrome, xpath: str) -> bool:\n",
    "    try:\n",
    "        el = driver.find_element(By.XPATH, xpath)\n",
    "        el.click()\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def ensure_stops_panel_open(driver: webdriver.Chrome) -> None:\n",
    "    # Intenta abrir panel \"Paradas/Paraderos/Stops\" y \"Recorrido/Route\" si hay tabs\n",
    "    # Se prueban varios textos comunes\n",
    "    candidates = [\n",
    "        \"//button[contains(translate(., 'PRDSETOA', 'prdsetoa'), 'paradas')]\",\n",
    "        \"//button[contains(translate(., 'PARADEROS', 'paraderos'), 'paraderos')]\",\n",
    "        \"//button[contains(translate(., 'STOPS', 'stops'), 'stops')]\",\n",
    "        \"//a[contains(translate(., 'PARADAS', 'paradas'), 'paradas')]\",\n",
    "        \"//a[contains(translate(., 'PARADEROS', 'paraderos'), 'paraderos')]\",\n",
    "        \"//a[contains(translate(., 'STOPS', 'stops'), 'stops')]\",\n",
    "    ]\n",
    "    for xp in candidates:\n",
    "        if click_if_present(driver, xp):\n",
    "            time.sleep(0.4)\n",
    "            break\n",
    "\n",
    "def scroll_to_bottom(driver: webdriver.Chrome, max_secs: float = 4.0):\n",
    "    start = time.time()\n",
    "    last = 0\n",
    "    while time.time() - start < max_secs:\n",
    "        cur = driver.execute_script(\"return document.body.scrollHeight || 0;\")\n",
    "        if cur == last:\n",
    "            break\n",
    "        last = cur\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(0.3)\n",
    "\n",
    "# -----------------------------\n",
    "# Parseadores\n",
    "# -----------------------------\n",
    "def soup_from_html(html: str) -> BeautifulSoup:\n",
    "    return BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "def extract_route_id_from_canonical(soup: BeautifulSoup) -> Optional[str]:\n",
    "    link = soup.find(\"link\", rel=\"canonical\")\n",
    "    if not link or not link.get(\"href\"):\n",
    "        return None\n",
    "    q = parse_qs(urlparse(link[\"href\"]).query)\n",
    "    return q.get(\"routes\", [None])[0]\n",
    "\n",
    "def text_or_none(el) -> Optional[str]:\n",
    "    try:\n",
    "        return el.get_text(\" \", strip=True)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_meta_from_route_soup(soup: BeautifulSoup, url: str) -> RouteData:\n",
    "    route_id = extract_route_id_from_canonical(soup)\n",
    "\n",
    "    # título/ref\n",
    "    title_el = soup.select_one(SEL_ROUTE_TITLE) or soup.select_one(SEL_ROUTE_TITLE_FALL)\n",
    "    title = text_or_none(title_el)\n",
    "    ref = None\n",
    "    if title:\n",
    "        m = re.search(r\"([A-Za-z\\-]+|\\d+)$\", title)\n",
    "        if m: ref = m.group(1)\n",
    "\n",
    "    # ciudad\n",
    "    city_el = soup.select_one(SEL_CITY_LABEL) or soup.select_one(SEL_CITY_LABEL_FALL)\n",
    "    city = text_or_none(city_el)\n",
    "\n",
    "    # última edición y stats\n",
    "    last_edit = text_or_none(soup.select_one(SEL_LAST_EDIT))\n",
    "    stats = [el.get_text(\" \", strip=True) for el in soup.select(SEL_STATS)]\n",
    "\n",
    "    # operador\n",
    "    operator = None\n",
    "    for dt in soup.select(\"dt\"):\n",
    "        if \"Operador\" in dt.get_text(\" \", strip=True) or \"Operator\" in dt.get_text(\" \", strip=True):\n",
    "            dd = dt.find_next(\"dd\")\n",
    "            operator = text_or_none(dd)\n",
    "            break\n",
    "\n",
    "    return RouteData(\n",
    "        route_id=route_id, ref=ref, name=title, operator=operator,\n",
    "        last_edit=last_edit, stats_raw=stats, city=city, url=url\n",
    "    )\n",
    "\n",
    "def extract_stops_from_dom(driver: webdriver.Chrome, base: str) -> List[RouteStop]:\n",
    "    stops: List[RouteStop] = []\n",
    "    blocks = driver.find_elements(By.CSS_SELECTOR, SEL_STOP_BLOCK)\n",
    "    # fallback si cambia el selector\n",
    "    if not blocks:\n",
    "        ensure_stops_panel_open(driver)\n",
    "        time.sleep(0.6)\n",
    "        blocks = driver.find_elements(By.CSS_SELECTOR, SEL_STOP_BLOCK)\n",
    "\n",
    "    for block in blocks:\n",
    "        try:\n",
    "            head_small = block.find_element(By.CSS_SELECTOR, SEL_STOP_HEAD_SMALL)\n",
    "            direction = head_small.text.strip()\n",
    "        except NoSuchElementException:\n",
    "            direction = \"\"\n",
    "        links = block.find_elements(By.CSS_SELECTOR, SEL_STOP_ITEM)\n",
    "        for i, a in enumerate(links, start=1):\n",
    "            href = a.get_attribute(\"href\") or \"\"\n",
    "            stop_id = href.rstrip(\"/\").split(\"/\")[-1] if href else None\n",
    "            name = a.text.strip()\n",
    "            stop_url = urljoin(base, href) if href else None\n",
    "            stops.append(RouteStop(\n",
    "                sequence=i, direction=direction, stop_id=stop_id,\n",
    "                stop_name=name, stop_url=stop_url\n",
    "            ))\n",
    "    return stops\n",
    "\n",
    "def try_extract_coords_from_html(html: str) -> List[Tuple[float, float]]:\n",
    "    # 1) Patrones LatLng(...)\n",
    "    coords: List[Tuple[float,float]] = []\n",
    "    for pat in COORD_PATTERNS:\n",
    "        for m in pat.finditer(html):\n",
    "            try:\n",
    "                lat = float(m.group(1))\n",
    "                lon = float(m.group(2)) if m.lastindex == 2 else float(m.group(3))\n",
    "                coords.append((lat, lon))\n",
    "            except Exception:\n",
    "                continue\n",
    "    # 2) Arrays grandes [[lat,lon], ...] dentro de scripts\n",
    "    bigs = []\n",
    "    for arrm in BIG_COORD_ARRAY.finditer(html):\n",
    "        text = arrm.group(0)\n",
    "        try:\n",
    "            arr = json.loads(text)\n",
    "            if isinstance(arr, list) and isinstance(arr[0], list) and len(arr) >= 6:\n",
    "                bigs.append(arr)\n",
    "        except Exception:\n",
    "            # si no es JSON válido, intentar eval seguro reemplazando\n",
    "            try:\n",
    "                text2 = text.replace(\" \", \"\")\n",
    "                text2 = re.sub(r\"([0-9])\\.(?=[0-9])\", r\"\\1.\", text2)\n",
    "                arr = json.loads(text2)\n",
    "                if isinstance(arr, list) and len(arr) >= 6:\n",
    "                    bigs.append(arr)\n",
    "            except Exception:\n",
    "                pass\n",
    "    # convertir a lat,lon tuples\n",
    "    for arr in bigs:\n",
    "        tmp = []\n",
    "        for p in arr:\n",
    "            if not (isinstance(p, list) and len(p) >= 2):\n",
    "                tmp = []; break\n",
    "            tmp.append((float(p[0]), float(p[1])))\n",
    "        if len(tmp) >= 6:\n",
    "            # elegir el mejor candidato por cercanía a Lima\n",
    "            # abs(lat) ~ 12, abs(lon) ~ 77; si invertido, swap\n",
    "            def looks_like_lima(lat, lon):\n",
    "                return 10 < abs(lat) < 14 and 74 < abs(lon) < 80\n",
    "            good = sum(looks_like_lima(lat, lon) for lat, lon in tmp)\n",
    "            if good < len(tmp) // 2:\n",
    "                # probar swap\n",
    "                tmp_sw = [(lon, lat) for lat, lon in tmp]\n",
    "                good_sw = sum(looks_like_lima(lat, lon) for lat, lon in tmp_sw)\n",
    "                if good_sw > good:\n",
    "                    tmp = tmp_sw\n",
    "            # añadir a coords pero marcando que es de una polilínea\n",
    "            return tmp\n",
    "    return coords\n",
    "\n",
    "def enrich_stops_with_coords(driver: webdriver.Chrome, stops: List[RouteStop], out_dir: Path,\n",
    "                             delay: float = REQUEST_DELAY_S) -> None:\n",
    "    html_dir = out_dir / \"stops_html\"\n",
    "    html_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for s in stops:\n",
    "        if not s.stop_url:\n",
    "            continue\n",
    "        try:\n",
    "            driver.get(s.stop_url)\n",
    "            time.sleep(0.7)\n",
    "            html = driver.page_source\n",
    "            # guardar HTML de la parada\n",
    "            fname = f\"stop_{s.stop_id or s.sequence}.html\"\n",
    "            (html_dir / fname).write_text(html, encoding=\"utf-8\", errors=\"ignore\")\n",
    "            # coordenadas\n",
    "            found = try_extract_coords_from_html(html)\n",
    "            if found:\n",
    "                # tomar la primera si hay varias en la página de la parada\n",
    "                lat, lon = found[0]\n",
    "                s.lat, s.lon = lat, lon\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(delay)\n",
    "\n",
    "# -----------------------------\n",
    "# Guardado\n",
    "# -----------------------------\n",
    "def save_route_outputs(out_dir: Path, route: RouteData, stops: List[RouteStop],\n",
    "                       page_html: str, map_coords: List[Tuple[float,float]],\n",
    "                       assets_urls: List[str], download_assets_flag: bool) -> None:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # HTML principal y assets\n",
    "    (out_dir / \"page.html\").write_text(page_html, encoding=\"utf-8\", errors=\"ignore\")\n",
    "    (out_dir / \"assets_links.txt\").write_text(\"\\n\".join(assets_urls), encoding=\"utf-8\")\n",
    "    if download_assets_flag:\n",
    "        download_assets(assets_urls, out_dir / \"assets\")\n",
    "\n",
    "    # route.json\n",
    "    (out_dir / \"route.json\").write_text(\n",
    "        json.dumps(asdict(route), ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    # stops.csv\n",
    "    with open(out_dir / \"stops.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"sequence\",\"direction\",\"stop_id\",\"stop_name\",\"stop_url\",\"lat\",\"lon\"])\n",
    "        for s in stops:\n",
    "            wr.writerow([s.sequence, s.direction, s.stop_id, s.stop_name, s.stop_url, s.lat, s.lon])\n",
    "\n",
    "    # stops.geojson\n",
    "    gj_pts = {\"type\":\"FeatureCollection\",\"features\":[]}\n",
    "    for s in stops:\n",
    "        if s.lat is None or s.lon is None:\n",
    "            continue\n",
    "        gj_pts[\"features\"].append({\n",
    "            \"type\":\"Feature\",\n",
    "            \"geometry\":{\"type\":\"Point\",\"coordinates\":[s.lon, s.lat]},\n",
    "            \"properties\":{\n",
    "                \"sequence\": s.sequence,\n",
    "                \"direction\": s.direction,\n",
    "                \"stop_id\": s.stop_id,\n",
    "                \"stop_name\": s.stop_name\n",
    "            }\n",
    "        })\n",
    "    (out_dir / \"stops.geojson\").write_text(json.dumps(gj_pts, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "    # line_approx.geojson: preferir polyline detectada; si no, unir paraderos por orden\n",
    "    features = []\n",
    "    used_coords = []\n",
    "\n",
    "    if map_coords and len(map_coords) >= 2:\n",
    "        # map_coords viene como [(lat,lon), ...] -> pasamos a [lon,lat]\n",
    "        coords = [[c[1], c[0]] for c in map_coords]\n",
    "        used_coords = coords\n",
    "        features.append({\n",
    "            \"type\":\"Feature\",\n",
    "            \"geometry\":{\"type\":\"LineString\",\"coordinates\":coords},\n",
    "            \"properties\":{\"source\": \"inline_polyline\"}\n",
    "        })\n",
    "\n",
    "        # para debug\n",
    "        (out_dir / \"raw_coords.json\").write_text(\n",
    "            json.dumps({\"points\": map_coords}, ensure_ascii=False, indent=2),\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "\n",
    "    if not features:\n",
    "        dirs: Dict[str, List[Tuple[float,float]]] = {}\n",
    "        for s in sorted([x for x in stops if x.lat is not None and x.lon is not None], key=lambda z: (z.direction, z.sequence)):\n",
    "            dirs.setdefault(s.direction or \"\", []).append((s.lon, s.lat))\n",
    "        for d, coords in dirs.items():\n",
    "            if len(coords) >= 2:\n",
    "                used_coords.extend(coords)\n",
    "                features.append({\n",
    "                    \"type\":\"Feature\",\n",
    "                    \"geometry\":{\"type\":\"LineString\",\"coordinates\":coords},\n",
    "                    \"properties\":{\"direction\": d or \"both\", \"source\":\"stops_chaining\"}\n",
    "                })\n",
    "\n",
    "    (out_dir / \"line_approx.geojson\").write_text(\n",
    "        json.dumps({\"type\":\"FeatureCollection\",\"features\":features}, ensure_ascii=False),\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# Scrapeadores\n",
    "# -----------------------------\n",
    "def scrape_route(base: str, url: str, out_root: Path, headless: bool = True,\n",
    "                 download_assets_flag: bool = False, delay: float = REQUEST_DELAY_S) -> Path:\n",
    "    if not robots_allows(base, urlparse(url).path):\n",
    "        raise RuntimeError(\"Robots.txt no permite scrapear esta ruta\")\n",
    "\n",
    "    driver = make_driver(headless=headless)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # intentar asegurar panel de paradas abierto y contenido cargado\n",
    "        ensure_stops_panel_open(driver)\n",
    "        scroll_to_bottom(driver, max_secs=4.0)\n",
    "        try:\n",
    "            wait_css(driver, SEL_STOP_BLOCK, timeout=PAGE_TIMEOUT_S)\n",
    "        except TimeoutException:\n",
    "            pass  # seguimos igual: guardaremos page.html para inspección\n",
    "\n",
    "        # HTML y soup\n",
    "        page_html = driver.page_source\n",
    "        soup = soup_from_html(page_html)\n",
    "\n",
    "        # metadatos\n",
    "        route = extract_meta_from_route_soup(soup, url)\n",
    "\n",
    "        # assets\n",
    "        assets_urls = collect_asset_urls(soup, url)\n",
    "\n",
    "        # paraderos\n",
    "        stops = extract_stops_from_dom(driver, base)\n",
    "        out_dir = out_root / f\"route_{route.route_id or re.sub(r'[^A-Za-z0-9_\\-]+','_', route.ref or 'ruta')}\"\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # coords de la polilínea en la página principal (si existen)\n",
    "        map_coords = try_extract_coords_from_html(page_html)\n",
    "\n",
    "        # enriquecer paraderos con lat/lon y guardar HTML de cada uno\n",
    "        enrich_stops_with_coords(driver, stops, out_dir, delay=delay)\n",
    "\n",
    "        # guardar todo\n",
    "        save_route_outputs(out_dir, route, stops, page_html, map_coords, assets_urls, download_assets_flag)\n",
    "\n",
    "        print(f\"OK: {route.name or route.ref} -> {out_dir}\")\n",
    "        return out_dir\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def find_route_links_in_catalog_page(driver: webdriver.Chrome, url: str, base: str) -> List[str]:\n",
    "    driver.get(url)\n",
    "    time.sleep(1.2)\n",
    "    # intentar lazy load\n",
    "    scroll_to_bottom(driver, max_secs=3.0)\n",
    "    anchors = driver.find_elements(By.CSS_SELECTOR, \"a\")\n",
    "    links = []\n",
    "    for a in anchors:\n",
    "        href = a.get_attribute(\"href\") or \"\"\n",
    "        if not href:\n",
    "            continue\n",
    "        if \"?routes=\" in href or re.search(r\"/routes/\\d+\", href):\n",
    "            links.append(href)\n",
    "    # fallback por texto de número corto\n",
    "    if not links:\n",
    "        for a in anchors:\n",
    "            txt = (a.text or \"\").strip()\n",
    "            if re.fullmatch(r\"[A-Za-z0-9\\-]{1,8}\", txt):\n",
    "                href = a.get_attribute(\"href\") or \"\"\n",
    "                if href:\n",
    "                    links.append(href)\n",
    "    base_netloc = urlparse(base).netloc\n",
    "    links = [u for u in links if urlparse(u).netloc == base_netloc]\n",
    "    return sorted(set(links))\n",
    "\n",
    "def scrape_catalog(base: str, city: str, lang: str, out_root: Path, max_pages: int = 30,\n",
    "                   headless: bool = True, download_assets_flag: bool = False, delay: float = REQUEST_DELAY_S) -> None:\n",
    "    first_cat_path = f\"/{lang}/{city}/catalog\"\n",
    "    if not robots_allows(base, first_cat_path):\n",
    "        raise RuntimeError(\"Robots.txt no permite scrapear el catálogo\")\n",
    "\n",
    "    driver = make_driver(headless=headless)\n",
    "    try:\n",
    "        seen = set()\n",
    "        for page in range(1, max_pages + 1):\n",
    "            cat_url = f\"{base}/{lang}/{city}/catalog?page={page}\"\n",
    "            print(f\"Catálogo p{page}: {cat_url}\")\n",
    "            links = find_route_links_in_catalog_page(driver, cat_url, base)\n",
    "            new_links = [u for u in links if u not in seen]\n",
    "            if not new_links:\n",
    "                print(\"No se encontraron más rutas en esta página. Deteniendo.\")\n",
    "                break\n",
    "            for u in new_links:\n",
    "                seen.add(u)\n",
    "                try:\n",
    "                    print(f\"Ruta: {u}\")\n",
    "                    scrape_route(base, u, out_root, headless=headless,\n",
    "                                 download_assets_flag=download_assets_flag, delay=delay)\n",
    "                except Exception as e:\n",
    "                    print(f\"[Aviso] Ruta fallida {u}: {e}\")\n",
    "                time.sleep(delay)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers para uso desde kernel\n",
    "# -----------------------------\n",
    "def run_route(url: str, out: str = \"data_wikiroutes\", base: str = DEFAULT_BASE,\n",
    "              headless: bool = True, download_assets: bool = False, delay: float = REQUEST_DELAY_S):\n",
    "    out_root = Path(out); out_root.mkdir(parents=True, exist_ok=True)\n",
    "    return scrape_route(base.rstrip(\"/\"), url, out_root, headless=headless,\n",
    "                        download_assets_flag=download_assets, delay=delay)\n",
    "\n",
    "def run_catalog(city: str = \"lima\", lang: str = \"es\", out: str = \"data_wikiroutes\", base: str = DEFAULT_BASE,\n",
    "                max_pages: int = 30, headless: bool = True, download_assets: bool = False, delay: float = REQUEST_DELAY_S):\n",
    "    out_root = Path(out); out_root.mkdir(parents=True, exist_ok=True)\n",
    "    return scrape_catalog(base.rstrip(\"/\"), city.strip(\"/\"), lang.strip(\"/\"), out_root,\n",
    "                          max_pages=max_pages, headless=headless,\n",
    "                          download_assets_flag=download_assets, delay=delay)\n",
    "\n",
    "# -----------------------------\n",
    "# CLI\n",
    "# -----------------------------\n",
    "def strip_ipykernel_args(argv):\n",
    "    if not argv:\n",
    "        return []\n",
    "    cleaned, skip = [], False\n",
    "    for a in argv:\n",
    "        if skip:\n",
    "            skip = False\n",
    "            continue\n",
    "        if a in (\"-f\", \"--f\", \"--file\"):\n",
    "            skip = True\n",
    "            continue\n",
    "        if a.startswith(\"--f=\") or a.startswith(\"--file=\") or a.startswith(\"-f=\"):\n",
    "            continue\n",
    "        cleaned.append(a)\n",
    "    return cleaned\n",
    "\n",
    "def main(argv=None):\n",
    "    ap = argparse.ArgumentParser(description=\"Scraper Selenium para WikiRoutes (scrapeo total)\")\n",
    "    sub = ap.add_subparsers(dest=\"cmd\")\n",
    "\n",
    "    cat = sub.add_parser(\"catalog\", help=\"Recorre el catálogo de una ciudad\")\n",
    "    cat.add_argument(\"--base\", default=DEFAULT_BASE)\n",
    "    cat.add_argument(\"--city\", default=\"lima\")\n",
    "    cat.add_argument(\"--lang\", default=\"es\")\n",
    "    cat.add_argument(\"--out\", default=\"data_wikiroutes\")\n",
    "    cat.add_argument(\"--max-pages\", type=int, default=30)\n",
    "    cat.add_argument(\"--headless\", type=int, default=1)\n",
    "    cat.add_argument(\"--download-assets\", type=int, default=0)\n",
    "    cat.add_argument(\"--delay\", type=float, default=REQUEST_DELAY_S)\n",
    "\n",
    "    rt = sub.add_parser(\"route\", help=\"Scrapea una ruta específica\")\n",
    "    rt.add_argument(\"--base\", default=DEFAULT_BASE)\n",
    "    rt.add_argument(\"--url\", required=True)\n",
    "    rt.add_argument(\"--out\", default=\"data_wikiroutes\")\n",
    "    rt.add_argument(\"--headless\", type=int, default=1)\n",
    "    rt.add_argument(\"--download-assets\", type=int, default=0)\n",
    "    rt.add_argument(\"--delay\", type=float, default=REQUEST_DELAY_S)\n",
    "\n",
    "    args = ap.parse_args(strip_ipykernel_args(argv))\n",
    "\n",
    "    if not args.cmd:\n",
    "        print(\"Uso:\")\n",
    "        print(\"  python wikiroutes_selenium.py catalog --city lima --lang es --out data_wikiroutes --download-assets 1\")\n",
    "        print(\"  python wikiroutes_selenium.py route --url 'https://wikiroutes.info/es/lima?routes=154193' --out data_wikiroutes --download-assets 1\")\n",
    "        return\n",
    "\n",
    "    out_root = Path(args.out); out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if args.cmd == \"catalog\":\n",
    "        scrape_catalog(args.base.rstrip(\"/\"), args.city.strip(\"/\"), args.lang.strip(\"/\"), out_root,\n",
    "                       max_pages=args.max_pages, headless=bool(args.headless),\n",
    "                       download_assets_flag=bool(args.download_assets), delay=float(args.delay))\n",
    "    elif args.cmd == \"route\":\n",
    "        scrape_route(args.base.rstrip(\"/\"), args.url, out_root,\n",
    "                     headless=bool(args.headless), download_assets_flag=bool(args.download_assets),\n",
    "                     delay=float(args.delay))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    main(sys.argv[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f2908d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: Autobús   1244 -> data_wikiroutes\\route_154193\n"
     ]
    }
   ],
   "source": [
    "# Abre Chrome visible para que veas qué pasa\n",
    "!python wikiroutes_selenium.py route --url \"https://wikiroutes.info/es/lima?routes=154193\" --out data_wikiroutes --download-assets 1 --headless 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a107b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta individual\n",
    "out_dir = run_route(\"https://wikiroutes.info/es/lima?routes=154193\", out=\"data_wikiroutes\")\n",
    "out_dir\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
