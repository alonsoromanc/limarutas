{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94a284ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Using cached selenium-4.39.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting webdriver-manager\n",
      "  Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\anaconda\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: requests in c:\\anaconda\\lib\\site-packages (2.32.3)\n",
      "Collecting urllib3<3.0,>=2.5.0 (from urllib3[socks]<3.0,>=2.5.0->selenium)\n",
      "  Using cached urllib3-2.6.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting trio<1.0,>=0.31.0 (from selenium)\n",
      "  Using cached trio-0.32.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
      "  Using cached trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting certifi>=2025.10.5 (from selenium)\n",
      "  Using cached certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting typing_extensions<5.0,>=4.15.0 (from selenium)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in c:\\anaconda\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\anaconda\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (24.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\anaconda\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\anaconda\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (3.7)\n",
      "Collecting outcome (from trio<1.0,>=0.31.0->selenium)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\anaconda\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\anaconda\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\anaconda\\lib\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.2)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\anaconda\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\anaconda\\lib\\site-packages (from webdriver-manager) (1.1.0)\n",
      "Requirement already satisfied: packaging in c:\\anaconda\\lib\\site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\anaconda\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\anaconda\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: pycparser in c:\\anaconda\\lib\\site-packages (from cffi>=1.14->trio<1.0,>=0.31.0->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.16.0 in c:\\anaconda\\lib\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
      "Using cached selenium-4.39.0-py3-none-any.whl (9.7 MB)\n",
      "Using cached trio-0.32.0-py3-none-any.whl (512 kB)\n",
      "Using cached trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached urllib3-2.6.1-py3-none-any.whl (131 kB)\n",
      "Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Using cached certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: urllib3, typing_extensions, outcome, certifi, trio, webdriver-manager, trio-websocket, selenium\n",
      "\n",
      "  Attempting uninstall: urllib3\n",
      "\n",
      "    Found existing installation: urllib3 2.3.0\n",
      "\n",
      "    Uninstalling urllib3-2.3.0:\n",
      "\n",
      "      Successfully uninstalled urllib3-2.3.0\n",
      "\n",
      "   ---------------------------------------- 0/8 [urllib3]\n",
      "   ---------------------------------------- 0/8 [urllib3]\n",
      "  Attempting uninstall: typing_extensions\n",
      "   ---------------------------------------- 0/8 [urllib3]\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "   ---------------------------------------- 0/8 [urllib3]\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "   ---------------------------------------- 0/8 [urllib3]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "   ----- ---------------------------------- 1/8 [typing_extensions]\n",
      "   ---------- ----------------------------- 2/8 [outcome]\n",
      "  Attempting uninstall: certifi\n",
      "   ---------- ----------------------------- 2/8 [outcome]\n",
      "    Found existing installation: certifi 2025.8.3\n",
      "   ---------- ----------------------------- 2/8 [outcome]\n",
      "    Uninstalling certifi-2025.8.3:\n",
      "   ---------- ----------------------------- 2/8 [outcome]\n",
      "      Successfully uninstalled certifi-2025.8.3\n",
      "   ---------- ----------------------------- 2/8 [outcome]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   -------------------- ------------------- 4/8 [trio]\n",
      "   ------------------------- -------------- 5/8 [webdriver-manager]\n",
      "   ------------------------------ --------- 6/8 [trio-websocket]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ----------------------------------- ---- 7/8 [selenium]\n",
      "   ---------------------------------------- 8/8 [selenium]\n",
      "\n",
      "Successfully installed certifi-2025.11.12 outcome-1.3.0.post0 selenium-4.39.0 trio-0.32.0 trio-websocket-0.12.2 typing_extensions-4.15.0 urllib3-2.6.1 webdriver-manager-4.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~rllib3 (c:\\Anaconda\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rllib3 (c:\\Anaconda\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~rllib3 (c:\\Anaconda\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install selenium webdriver-manager beautifulsoup4 requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9815393d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uso: wikiroutes_selenium.py [catalog|route] ...\n",
      "Ejemplos:\n",
      "  python wikiroutes_selenium.py catalog --city lima --lang es --out data_wikiroutes\n",
      "  python wikiroutes_selenium.py route --url 'https://wikiroutes.info/es/lima?routes=154193' --out data_wikiroutes\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "WikiRoutes scraper con Selenium.\n",
    "Genera por ruta:\n",
    "- route.json\n",
    "- stops.csv\n",
    "- stops.geojson\n",
    "- line_approx.geojson\n",
    "\n",
    "Requisitos:\n",
    "pip install selenium webdriver-manager beautifulsoup4 requests\n",
    "\"\"\"\n",
    "\n",
    "import re, csv, json, time, argparse\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, Dict, Any\n",
    "from urllib.parse import urlparse, urljoin, parse_qs\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "DEFAULT_BASE = \"https://wikiroutes.info\"\n",
    "UA = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \" \\\n",
    "     \"(KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\n",
    "REQUEST_DELAY_S = 1.2\n",
    "PAGE_TIMEOUT_S = 25\n",
    "\n",
    "# Selectores conocidos de WikiRoutes\n",
    "SEL_STOP_BLOCK = \".stops-list-block\"\n",
    "SEL_STOP_HEAD_SMALL = \".stops-list-block-head small\"\n",
    "SEL_STOP_ITEM = \".stops-list-item\"\n",
    "SEL_ROUTE_TITLE = \".MEcFqLPlaQKg.RSZfWQHoH\"\n",
    "SEL_CITY_LABEL = \".vGyZhDoaGCm.khuKVSRut\"\n",
    "SEL_LAST_EDIT = \".PNkzKgEzLcTwnP\"\n",
    "SEL_STATS = \".bLKuCSlgiB .MuinWLFvyLRChv\"\n",
    "\n",
    "COORD_PATTERNS = [\n",
    "    re.compile(r\"LatLng\\(\\s*([-0-9\\.]+)\\s*,\\s*([-0-9\\.]+)\\s*\\)\", re.I),\n",
    "    re.compile(r\"['\\\"]lat['\\\"]\\s*[:=]\\s*([-0-9\\.]+)\\s*,\\s*['\\\"](lon|lng)['\\\"]\\s*[:=]\\s*([-0-9\\.]+)\", re.I),\n",
    "    re.compile(r\"data-lat=\\\"([-0-9\\.]+)\\\".*?data-lon=\\\"([-0-9\\.]+)\\\"\", re.I | re.S),\n",
    "]\n",
    "\n",
    "@dataclass\n",
    "class RouteStop:\n",
    "    sequence: int\n",
    "    direction: str\n",
    "    stop_id: Optional[str]\n",
    "    stop_name: str\n",
    "    stop_url: Optional[str]\n",
    "    lat: Optional[float] = None\n",
    "    lon: Optional[float] = None\n",
    "\n",
    "@dataclass\n",
    "class RouteData:\n",
    "    route_id: Optional[str]\n",
    "    ref: Optional[str]\n",
    "    name: Optional[str]\n",
    "    operator: Optional[str]\n",
    "    last_edit: Optional[str]\n",
    "    stats_raw: List[str]\n",
    "    city: Optional[str]\n",
    "    url: str\n",
    "\n",
    "def robots_allows(base: str, path: str) -> bool:\n",
    "    try:\n",
    "        r = requests.get(urljoin(base, \"/robots.txt\"), timeout=10)\n",
    "        if r.status_code != 200:\n",
    "            return True\n",
    "        ua = \"*\"\n",
    "        dis = []\n",
    "        current = None\n",
    "        for line in r.text.splitlines():\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "            if line.lower().startswith(\"user-agent:\"):\n",
    "                current = line.split(\":\", 1)[1].strip()\n",
    "            elif line.lower().startswith(\"disallow:\") and (current == ua or current == \"*\"):\n",
    "                dis.append(line.split(\":\", 1)[1].strip())\n",
    "        return not any(path.startswith(d) for d in dis if d)\n",
    "    except Exception:\n",
    "        return True\n",
    "\n",
    "def make_driver(headless: bool = True, lang: str = \"es-ES\") -> webdriver.Chrome:\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(f\"--user-agent={UA}\")\n",
    "    opts.add_argument(f\"--lang={lang}\")\n",
    "    opts.add_argument(\"--window-size=1360,900\")\n",
    "    # Evitar detecciones triviales\n",
    "    opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)\n",
    "    driver.set_page_load_timeout(PAGE_TIMEOUT_S)\n",
    "    return driver\n",
    "\n",
    "def page_wait_css(driver: webdriver.Chrome, css: str, timeout: int = PAGE_TIMEOUT_S):\n",
    "    WebDriverWait(driver, timeout).until(EC.presence_of_element_located((By.CSS_SELECTOR, css)))\n",
    "\n",
    "def text_or_none(el) -> Optional[str]:\n",
    "    try:\n",
    "        return el.text.strip()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def soup_from_html(html: str) -> BeautifulSoup:\n",
    "    return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "def extract_route_id_from_canonical(soup: BeautifulSoup) -> Optional[str]:\n",
    "    link = soup.find(\"link\", rel=\"canonical\")\n",
    "    if not link or not link.get(\"href\"):\n",
    "        return None\n",
    "    q = parse_qs(urlparse(link[\"href\"]).query)\n",
    "    return q.get(\"routes\", [None])[0]\n",
    "\n",
    "def extract_meta_from_route_soup(soup: BeautifulSoup, url: str) -> RouteData:\n",
    "    route_id = extract_route_id_from_canonical(soup)\n",
    "    city = text_or_none(soup.select_one(SEL_CITY_LABEL))\n",
    "    title = text_or_none(soup.select_one(SEL_ROUTE_TITLE))\n",
    "    last_edit = text_or_none(soup.select_one(SEL_LAST_EDIT))\n",
    "    stats = [el.get_text(\" \", strip=True) for el in soup.select(SEL_STATS)]\n",
    "    ref, name = None, None\n",
    "    if title:\n",
    "        m = re.search(r\"([A-Za-z\\-]+|\\d+)$\", title)\n",
    "        if m:\n",
    "            ref = m.group(1)\n",
    "        name = title\n",
    "    operator = None\n",
    "    for dt in soup.select(\"dt\"):\n",
    "        if \"Operador\" in dt.get_text(\" \", strip=True):\n",
    "            dd = dt.find_next(\"dd\")\n",
    "            operator = dd.get_text(\" \", strip=True) if dd else None\n",
    "            break\n",
    "    return RouteData(\n",
    "        route_id=route_id, ref=ref, name=name, operator=operator,\n",
    "        last_edit=last_edit, stats_raw=stats, city=city, url=url\n",
    "    )\n",
    "\n",
    "def extract_stops_from_route_dom(driver: webdriver.Chrome, base: str) -> List[RouteStop]:\n",
    "    stops: List[RouteStop] = []\n",
    "    blocks = driver.find_elements(By.CSS_SELECTOR, SEL_STOP_BLOCK)\n",
    "    for block in blocks:\n",
    "        try:\n",
    "            head_small = block.find_element(By.CSS_SELECTOR, SEL_STOP_HEAD_SMALL)\n",
    "            direction = head_small.text.strip()\n",
    "        except NoSuchElementException:\n",
    "            direction = \"\"\n",
    "        links = block.find_elements(By.CSS_SELECTOR, SEL_STOP_ITEM)\n",
    "        for i, a in enumerate(links, start=1):\n",
    "            href = a.get_attribute(\"href\") or \"\"\n",
    "            stop_id = href.rstrip(\"/\").split(\"/\")[-1] if href else None\n",
    "            name = a.text.strip()\n",
    "            stop_url = urljoin(base, href) if href else None\n",
    "            stops.append(RouteStop(\n",
    "                sequence=i, direction=direction, stop_id=stop_id,\n",
    "                stop_name=name, stop_url=stop_url\n",
    "            ))\n",
    "    return stops\n",
    "\n",
    "def try_extract_coords_from_html(html: str) -> Optional[Tuple[float, float]]:\n",
    "    for pat in COORD_PATTERNS:\n",
    "        m = pat.search(html)\n",
    "        if m:\n",
    "            try:\n",
    "                lat = float(m.group(1))\n",
    "                lon = float(m.group(2)) if m.lastindex == 2 else float(m.group(3))\n",
    "                return lat, lon\n",
    "            except Exception:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "def enrich_stops_with_coords(driver: webdriver.Chrome, stops: List[RouteStop], delay: float = REQUEST_DELAY_S) -> None:\n",
    "    for s in stops:\n",
    "        if not s.stop_url:\n",
    "            continue\n",
    "        try:\n",
    "            driver.get(s.stop_url)\n",
    "            # una espera corta ayuda si hay JS\n",
    "            time.sleep(0.5)\n",
    "            html = driver.page_source\n",
    "            coords = try_extract_coords_from_html(html)\n",
    "            if coords:\n",
    "                s.lat, s.lon = coords\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(delay)\n",
    "\n",
    "def save_route_outputs(out_dir: Path, route: RouteData, stops: List[RouteStop]) -> None:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (out_dir / \"route.json\").write_text(json.dumps(asdict(route), ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    with open(out_dir / \"stops.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"sequence\",\"direction\",\"stop_id\",\"stop_name\",\"stop_url\",\"lat\",\"lon\"])\n",
    "        for s in stops:\n",
    "            wr.writerow([s.sequence, s.direction, s.stop_id, s.stop_name, s.stop_url, s.lat, s.lon])\n",
    "\n",
    "    # GeoJSON de puntos\n",
    "    gj_pts = {\"type\":\"FeatureCollection\",\"features\":[]}\n",
    "    for s in stops:\n",
    "        if s.lat is None or s.lon is None:\n",
    "            continue\n",
    "        gj_pts[\"features\"].append({\n",
    "            \"type\":\"Feature\",\n",
    "            \"geometry\":{\"type\":\"Point\",\"coordinates\":[s.lon, s.lat]},\n",
    "            \"properties\":{\n",
    "                \"sequence\": s.sequence,\n",
    "                \"direction\": s.direction,\n",
    "                \"stop_id\": s.stop_id,\n",
    "                \"stop_name\": s.stop_name\n",
    "            }\n",
    "        })\n",
    "    (out_dir / \"stops.geojson\").write_text(json.dumps(gj_pts, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "    # LineString por sentido si hay al menos 2 puntos\n",
    "    dirs: Dict[str, List[Tuple[float,float]]] = {}\n",
    "    for s in sorted([x for x in stops if x.lat is not None and x.lon is not None], key=lambda z: z.sequence):\n",
    "        dirs.setdefault(s.direction or \"\", []).append((s.lon, s.lat))\n",
    "    features = []\n",
    "    for d, coords in dirs.items():\n",
    "        if len(coords) >= 2:\n",
    "            features.append({\n",
    "                \"type\":\"Feature\",\n",
    "                \"geometry\":{\"type\":\"LineString\",\"coordinates\":coords},\n",
    "                \"properties\":{\"direction\": d}\n",
    "            })\n",
    "    (out_dir / \"line_approx.geojson\").write_text(json.dumps({\"type\":\"FeatureCollection\",\"features\":features}, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "def scrape_route(base: str, url: str, out_root: Path, headless: bool = True) -> Path:\n",
    "    if not robots_allows(base, urlparse(url).path):\n",
    "        raise RuntimeError(\"Robots.txt no permite scrapear esta ruta\")\n",
    "    driver = make_driver(headless=headless)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        page_wait_css(driver, SEL_STOP_BLOCK)\n",
    "        html = driver.page_source\n",
    "        soup = soup_from_html(html)\n",
    "        route = extract_meta_from_route_soup(soup, url)\n",
    "        stops = extract_stops_from_route_dom(driver, base)\n",
    "        enrich_stops_with_coords(driver, stops, delay=REQUEST_DELAY_S)\n",
    "\n",
    "        rid = route.route_id or re.sub(r\"[^A-Za-z0-9_\\-]+\", \"_\", route.ref or \"ruta\")\n",
    "        out_dir = out_root / f\"route_{rid}\"\n",
    "        save_route_outputs(out_dir, route, stops)\n",
    "        return out_dir\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def find_route_links_in_catalog_page(driver: webdriver.Chrome, url: str, base: str) -> List[str]:\n",
    "    driver.get(url)\n",
    "    time.sleep(1.2)\n",
    "    anchors = driver.find_elements(By.CSS_SELECTOR, \"a\")\n",
    "    links = []\n",
    "    for a in anchors:\n",
    "        href = a.get_attribute(\"href\") or \"\"\n",
    "        if not href:\n",
    "            continue\n",
    "        if \"?routes=\" in href or re.search(r\"/routes/\\d+\", href):\n",
    "            links.append(href)\n",
    "    # fallback: texto que parezca número de ruta\n",
    "    if not links:\n",
    "        for a in anchors:\n",
    "            txt = (a.text or \"\").strip()\n",
    "            if re.fullmatch(r\"[A-Za-z0-9\\-]{1,8}\", txt):\n",
    "                href = a.get_attribute(\"href\") or \"\"\n",
    "                if href:\n",
    "                    links.append(href)\n",
    "    # normaliza y filtra dominio\n",
    "    base_netloc = urlparse(base).netloc\n",
    "    links = [u for u in links if urlparse(u).netloc == base_netloc]\n",
    "    return sorted(set(links))\n",
    "\n",
    "def scrape_catalog(base: str, city: str, lang: str, out_root: Path, max_pages: int = 30, headless: bool = True) -> None:\n",
    "    first_cat_path = f\"/{lang}/{city}/catalog\"\n",
    "    if not robots_allows(base, first_cat_path):\n",
    "        raise RuntimeError(\"Robots.txt no permite scrapear el catálogo\")\n",
    "    driver = make_driver(headless=headless)\n",
    "    try:\n",
    "        seen = set()\n",
    "        for page in range(1, max_pages + 1):\n",
    "            cat_url = f\"{base}/{lang}/{city}/catalog?page={page}\"\n",
    "            print(f\"Catálogo p{page}: {cat_url}\")\n",
    "            links = find_route_links_in_catalog_page(driver, cat_url, base)\n",
    "            new_links = [u for u in links if u not in seen]\n",
    "            if not new_links:\n",
    "                print(\"No se encontraron más rutas en esta página. Deteniendo.\")\n",
    "                break\n",
    "            for u in new_links:\n",
    "                seen.add(u)\n",
    "                try:\n",
    "                    print(f\"Ruta: {u}\")\n",
    "                    scrape_route(base, u, out_root, headless=headless)\n",
    "                except Exception as e:\n",
    "                    print(f\"[Aviso] Ruta fallida {u}: {e}\")\n",
    "                time.sleep(REQUEST_DELAY_S)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "# Helpers Jupyter\n",
    "def run_route(url: str, out: str = \"data_wikiroutes\", base: str = DEFAULT_BASE, headless: bool = True):\n",
    "    out_root = Path(out); out_root.mkdir(parents=True, exist_ok=True)\n",
    "    return scrape_route(base.rstrip(\"/\"), url, out_root, headless=headless)\n",
    "\n",
    "def run_catalog(city: str = \"lima\", lang: str = \"es\", out: str = \"data_wikiroutes\", base: str = DEFAULT_BASE, max_pages: int = 30, headless: bool = True):\n",
    "    out_root = Path(out); out_root.mkdir(parents=True, exist_ok=True)\n",
    "    return scrape_catalog(base.rstrip(\"/\"), city.strip(\"/\"), lang.strip(\"/\"), out_root, max_pages=max_pages, headless=headless)\n",
    "\n",
    "# CLI\n",
    "def strip_ipykernel_args(argv):\n",
    "    if not argv:\n",
    "        return []\n",
    "    cleaned, skip = [], False\n",
    "    for a in argv:\n",
    "        if skip:\n",
    "            skip = False\n",
    "            continue\n",
    "        if a in (\"-f\", \"--f\", \"--file\"):\n",
    "            skip = True\n",
    "            continue\n",
    "        if a.startswith(\"--f=\") or a.startswith(\"--file=\") or a.startswith(\"-f=\"):\n",
    "            continue\n",
    "        cleaned.append(a)\n",
    "    return cleaned\n",
    "\n",
    "def main(argv=None):\n",
    "    ap = argparse.ArgumentParser(description=\"Scraper Selenium para WikiRoutes\")\n",
    "    sub = ap.add_subparsers(dest=\"cmd\")\n",
    "\n",
    "    cat = sub.add_parser(\"catalog\", help=\"Recorre el catálogo de una ciudad\")\n",
    "    cat.add_argument(\"--base\", default=DEFAULT_BASE)\n",
    "    cat.add_argument(\"--city\", default=\"lima\")\n",
    "    cat.add_argument(\"--lang\", default=\"es\")\n",
    "    cat.add_argument(\"--out\", default=\"data_wikiroutes\")\n",
    "    cat.add_argument(\"--max-pages\", type=int, default=30)\n",
    "    cat.add_argument(\"--headless\", type=int, default=1)\n",
    "\n",
    "    rt = sub.add_parser(\"route\", help=\"Scrapea una ruta específica\")\n",
    "    rt.add_argument(\"--base\", default=DEFAULT_BASE)\n",
    "    rt.add_argument(\"--url\", required=True)\n",
    "    rt.add_argument(\"--out\", default=\"data_wikiroutes\")\n",
    "    rt.add_argument(\"--headless\", type=int, default=1)\n",
    "\n",
    "    args = ap.parse_args(strip_ipykernel_args(argv))\n",
    "\n",
    "    if not args.cmd:\n",
    "        print(\"Uso: wikiroutes_selenium.py [catalog|route] ...\")\n",
    "        print(\"Ejemplos:\\n  python wikiroutes_selenium.py catalog --city lima --lang es --out data_wikiroutes\\n  python wikiroutes_selenium.py route --url 'https://wikiroutes.info/es/lima?routes=154193' --out data_wikiroutes\")\n",
    "        return\n",
    "\n",
    "    out_root = Path(args.out); out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if args.cmd == \"catalog\":\n",
    "        scrape_catalog(args.base.rstrip(\"/\"), args.city.strip(\"/\"), args.lang.strip(\"/\"), out_root, max_pages=args.max_pages, headless=bool(args.headless))\n",
    "    elif args.cmd == \"route\":\n",
    "        scrape_route(args.base.rstrip(\"/\"), args.url, out_root, headless=bool(args.headless))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    main(sys.argv[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f2908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abre Chrome visible para que veas qué pasa\n",
    "!python wikiroutes_selenium.py route --url \"https://wikiroutes.info/es/lima?routes=154193\" --out data_wikiroutes --headless 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a107b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta individual\n",
    "out_dir = run_route(\"https://wikiroutes.info/es/lima?routes=154193\", out=\"data_wikiroutes\")\n",
    "out_dir\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
